\chapter{Evaluation}\label{ch5}

The ecoap prototype implementation is used to evaluate the proposed system architecture for scalable and reliable IoT services. Benchmarks are performed against the state of the art CoAP implementation for both unconstrained and constrained environments. The evaluation scenarios, however, can vary depending on the many different communication models in IoT applications, including pure server, pure client and complex logic combining both of them \autocite{kovatsch2015scalable}. 

For instance, a service could consist of resource directories which are servers that manage the devices and provide discovery. The devices would first register themselves there by issuing a request and periodically update their status \autocite{core_directory}. Then other devices and services could contact the resource directory server for lookups. The devices can also be servers while a cloud service takes the role of client and observes resources hosted on many devices for monitoring and sensing. A combination of the two such as the OMA Lightweight M2M \autocite{lwm2m} specification acts as a resource directory and proxy at the same time, which means it not only receives registration and look-up requests from devices, but also issues requests to the resources of the devices. The devices in such case take both roles as well, since they expose their data as resources while using requests to register with the service.

To better evaluate the scalability and reliability of the proposed implementation, the pure server scenario is chosen because the other scenarios can have various concurrency requirements which are highly application dependent. Moreover, it allows for a direct comparison with the other mainstream CoAP implementation, the Californium (Cf) CoAP framework \autocite{californium}.

%One goal of Californium is to host services that communicate with other endpoints. In the IoT, we expect endpoints to exchange small messages. We consider each request and response a single unit of information. For the endpoints that communicate with a server, the relevant factors are the number of information that can be exchanged per time and the latency. Whether it is a CoAP or an HTTP message that carries the information from one endpoint to another is of no importance for the application. We measure the throughput of a server as the average number of requests that it is able to handle per second. This chapter presents the results of our experiments with Californium, Old Californium, and five state-of-the-art HTTP servers. Therefore this evaluation also serves as a comparison between CoAP and HTTP as protocols in the service backend. We particularly evaluate the scalability of the seven servers with respect to the number of available cores and with respect to the number of endpoints that concurrently communicate with the server. A node In the World Wide Web usually is either in the role of a server or a client. Therefore, HTTP servers truly are servers only and are optimized for that purpose. For the sake of comparison, we concentrate our evaluation of Californium and Old Californium only on their role as server even though both can appear as clients as well.

%We distinguish between two use-cases in which a server reaches its maximum performance. First, there might be a few clients that send many requests to the server. Such a client might be a proxy, for example, that in fact forwards requests from many clients but represents itself to the server as one highly demanding client. If server and clients used HTTP, they could keep a TCP connection alive and exchange many messages over it. In the second case, there are a large number of clients, each sending only a single request to the server. For an HTTP server, this means that each client establishes a new TCP connection only to exchange one request and immediately terminate it again. This is the scenario for a resource directory (RD). An RD can be thought of as an address book for resources. A endpoint can for instance register its resources of a specific type at the RD and another endpoint that looks for such a resource might later retrieve its URI form the RD. To simulate this scenario, HTTP clients do not keep their TCP connections alive but reestablish it for each request. The distinction between these two use-cases is much more prominent for HTTP than for CoAP as there is no such thing as establishing a connection in CoAP and it makes no difference whether requests come from the same or many different clients.

\section{Experiment Setup}

\subsection{Benchmark Tool}

It is considered that in a typical IoT scenario, many endpoints would exchange small messages with certain services, while each request and response is treated as a single, compact unit of information. Therefore the number of information that can be exchanged per time and latency are the most important factors for the measurement of a server.

However, there is few benchmark tool for CoAP measuring these propeties. The one used by Californium is CoAPBench \autocite{coapbench}, which is a Java based application similar to ApacheBench \autocite{apachebench} and can be distributed over multiple machines. It is decided to develop an Erlang counterpart which follows its test logic. This is for two purpose. Firstly, with the concurrency model of Erlang, the benchmark tool can achieve higher level of load on one node thus avoid using multiple machines and simplify the test process. Second and the most important, the original CoAPBench does not provide a clear latency tracking functionality which is necessary in this evaluation, while integrating one with CoAPBench is not a trivial task. It is easier to reuse the components of ecoap to satisfy such a requirement. 

Similar with CoAPBench, the Erlang benchmark tool uses virtual clients for concurrency factor. A virtual client is a simplified CoAP client which can be easily implemented as one Erlang process. It is argued that since CoAP is a connection-less protocol, it does not make a lot difference whether messages come from one endpoint or many endpoints. However the many virtual clients model shows more similarity to real world use cases since it not only simulates all independent endpoints but also obeys the stop-and-wait nature of the protocol. High message rates in the IoT usually comes from millions of devices sending occasionally with intervals in minute or hour, instead of a small number sending at very high rate constantly \autocite{kovatsch2015scalable}. More importantly, the fault-tolerance of ecoap is built based on the assumption that the server communicates with endpoints each having a different identification (e.g. address). Therefore the virtual clients model also helps verify the fault tolerance behaviour in the experiment. A brief architecture of the benchmark tool can be found in \autoref{fig:ecoap_bench_arch}.

\begin{figure}[!htbp]
\centering
\includegraphics[scale = 0.7]{ecoap_bench_arch}
\caption[The supervision tree of the Erlang CoAP benchmark tool]{The supervision tree of the Erlang CoAP benchmark tool. A virtual client is a simplified CoAP client which only sends request in a closed-loop fashion, that is, waiting for a response of the previous request before sending next one. Each virtual client holds a HDR histogram which records response time of every request. The bench server process holds another HDR histogram which aggregates the contents of all virtual clients after a test has finished.}
\label{fig:ecoap_bench_arch}
\end{figure}

The benchmark tool works in a closed-loop fashion and adheres to basic congestion control. That is to say, each virtual client sends requests to the server as fast as the server can handle them: it sends confirmable requests and always waits for the response before the next request is issued. Retransmission is disabled in order to not blur the number of sent and successfully handled requests. A timeout of 10 seconds is applied in case of a message loss, which is recorded in a separate counter. After that, the virtual client continues with a new request. Round trip time for each request is stored using an Erlang ports of the High Dynamic Range (HDR) Histogram \autocite{hdr_erl}, which supports the recording and analyzing of sampled data value with dynamic range and pre-defined precision in constant memory footprint. One can further specify the number of virtual clients used in one test run and how long the test lasts. Throughput is therefore calculated as the sum of  responses divided by the period of time of the test run. The benchmark tool reports the throughput, number of timeouts and statistically interesting latency measurements including minimum, maximum and percentile values after the test completes.

%While there are plenty of benchmark tools available for HTTP, to the best of our knowledge, there is none for CoAP. Therefore, we developed CoAPBench, a tool similar to ApacheBench31. It is part of our Californium framework, and hence publicly available to replicate our experiments.

%CoAPBench

%CoAPBench uses virtual clients to meet the defined concurrency factor. To have enough resources to saturate the server and keep all collected statistics in memory, CoAPBench can be distributed over multiple machines. A master controls the benchmark by establishing a TCP connection to all slave instances. We designed this master/slave mechanism to be able to execute third-party benchmark tools as well. Thus, we can run ApacheBench distributed and synchronized over multiple machines and bring even very powerful HTTP servers into saturation. Note that master and slaves only communicate before and after the experiment, so that the network traffic is not influenced by our tool.

%CoAPBench adheres to basic congestion control, that is, each CoAP client sends Confirmable requests and waits for the response before the next request is issued. We disable retransmissions, though, to not blur the numbers of sent and successfully handled requests. In case of message loss, a client times out after 10 seconds, records the loss in a separate counter, and continues with a new request.

\subsection{Setup}

The main interest of this work is the server performance on handling the protocol instead of complex business logic. Therefore the server under test simply holds a \verb|/benchmark| resource which responds with a piggybacked ``hello world" payload to confirmable GET requests. A growing number of concurrent clients are used to achieve desired concurrency factor, which increases stepwise from 10 to 10,000. All clients stress the server for 60 seconds with requests and then comes a 30 seconds cool-down interval.  Test for each concurrency factor is performed five times. Server throughput is presented as average value while latency for each test run is recorded separately.

All benchmarks are performed using the Erlang benchmark tool. The test platform varies and is given for the individual experiments respectively.

%Figure 4.9 depicts the setup in more detail and gives the machine specifications. The platform hosting the Web server under test varies and is given for the individual experiments.

\section{Multi-core Scalability} \label{multi_core_scalability}

This section presents the results of the experiments with ecoap and Californium (release 1.0.6), with focus on the investigation of scalability with respect to the number of concurrently active endpoints that communicate with the server and/or with respect to the number of available CPU cores. The experiments are conducted under unconstrained and constrained environment respectively. 

\subsection{Unconstrained Environment}

The evaluation under unconstrained environment is done by measuring the throughput and latency on virtual machines with increasing capability in a cloud. The unconstrained cloud is chosen as target platform because it should better reveal the scalability of the server compared with a constrained one, especially how well the server utilizes modern multi-core systems. Virtual machines hosted on Amazon Web Service (AWS) are used for the test since it is highly configurable. In order to reduce the disturbance in such a virtualized environment, all benchmarks run on dedicated instances \autocite{aws_dedicated_instance} where no other users could share the resource simultaneously. 

The benchmark tool is running on a m4.4xlarge instance with 16 vCPUs and 64 GB of RAM, while the server under test is running on an instance with increasing capability for each run, both with Ubuntu Server 16.04 LTS installed. As shown in \autoref{fig:experiment_setting_aws}, the initial configuration of the server host is a m4.large instance with 2 vCPUs and 8 GB of RAM. For further tests the host is improved to a m4.xlarge instance with 4 vCPUs and 16 GB of RAM, a m4.2xlarge instance with 8 vCPUs and 32 GB of RAM, and a m4.4xlarge instance with 16 vCPUs and 64 GB of RAM, respectively. The client instance and the server instance are connected to one subnet exclusively through enhanced networking (up to 10 Gbps). More detailed information of aforemetioned AWS instances can be found in \autocite{aws_instance}.

\begin{figure}[!htbp]
\centering
\includegraphics[scale = 0.8]{experiment_setting_aws}
\caption[Experiment setup in unconstrained environment]{Experiment setup in unconstrained environment: The platform for the system under test starts with AWS m4.large instance and is improved for each series of tests, up to AWS m4.4xlarge instance. In general each capability upgrade doubles available vCPUs and RAM.} 
\label{fig:experiment_setting_aws}
\end{figure}

Since the server has to cache requests and corresponding responses to detect duplicates, the memory consumption soon becomes the the majority of the load. Though CoAP allows to relax duplicate detection for idempotent requests, it is decided to strongly reduce the lifetime of confirmable requests rather than completely disable the deduplication, because the latter removes most state management pressure which makes the experiment even further from any real world use case. As a result, the EXCHANGE LIFETIME is tweaked to 1.5 seconds which leaves the server enough time to clean up all stale states during the interval between each test run, meanwhile ensuring the state management still be a part of the ordinary server activity.

Maximum number of open file descriptors is increased to allow enough active UDP sockets. The socket buffer is increased to 1 MB for both servers to reduce unnecessary message loss. The Erlang runtime is configured to bind all schedulers to available CPU cores and use kernel poll provided by the operating system. 

\begin{figure}[!htbp]
\centering
\begin{subfigure}{0.75\textwidth}
\includegraphics[width=\linewidth]{cf_vertical_throughput}
\caption{Californium}
\end{subfigure}
\begin{subfigure}{0.75\textwidth}
\includegraphics[width=\linewidth]{ecoap_vertical_throughput}
\caption{ecoap}
\end{subfigure}
\caption[Throughput on an AWS instance with increasing capability]{Throughput on an AWS instance with increasing capability: ecoap is less performant with lower configuration but scales better with higher configuration. Californium scales well under 8 cores but does not perform well beyond that. It also has higher variance.}
\label{fig:vertical_throughput}
\end{figure}

\autoref{fig:vertical_throughput} shows the throughput of the two servers with increasing capability of the underlying instance. The same scale is used for both figures. Overall message loss is less than 1\% and has negligible influence on the experiment. With 2 cores and 4 cores, Californium scales well and outperforms ecoap. Its maximum throughput is about 1.5 times higher on 4 cores than on 2 cores. Though ecoap doubles the throughput of itself when upgrading from 2 cores to 4 cores, it still can not go beyond Californium, which almost has 2 times better performance. However, when the configuration reaches 8 cores and higher, ecoap scales much better than Californium. Californium performs around 1.2 times better on 8 cores than 4 cores, but performs poorly on 16 cores. The throughput drops to a lower level than itself on 4 cores. On the other hand,  ecoap doubles its throughput from 4 cores to 8 cores and has a 1.4 times improvement from 8 cores to 16 cores. With 8 cores, maximum throughput for Californium and ecoap is 72,000 requests per second and 84,000 requests per second, respectively. While with 16 cores, the two numbers are 58,000 requests per second and 110,000 requests per second. It is expected that with the increasing number of available cores, improvements becomes less obvious since not all tasks can be well parallelized. With that being said, the above results still shows that ecoap has satisfying vertical scalability under unconstrained environment. 

One observation from the test is Californium performs better with lower configuration. It also stabilizes at around 100 concurrent clients, earlier than ecoap which usually stabilizes after 500 clients. This can be explained as the overhead brought by the Erlang runtime, which always strive to schedule all processes fairly. As a result, the server can not fully utilize the advantage of the runtime without enough available resources and workload. In contrast, Californium has configurable threading model which might fit in such situations well.

Another observation is that Californium has a higher standard deviation during the test, as indicated by the error bars in the figure. This may have many reasons. After observing the system resource consumption during the experiments, it is found that the Erlang runtime occupies more CPU and saturate faster than the Java Virtual Machine (JVM), while the JVM frees memory much slower than the Erlang one. As a result, Californium consumes much more memory than ecoap especially after long time testing. It is confirmed through both the Linux process viewer \verb|htop| and the \verb|jvisualvm| \autocite{jvisualvm} virtual machine profiling tool. There is no related memory leak being observed though, as local test proved that message exchange states are successfully removed after corresponding lifetime. It is inferred that the JVM needs more time to invoke a full garbage collection with a large RAM, meanwhile the high concurrency level leads to large amount of objects being created and deleted frequently, which eventually influenced the performance of the server. In addition, the virtual environment on AWS might have undesired impact on the experiment. In contrast, the Erlang VM uses a per-process based generational garbage collection strategy. As the name indicates, it runs inside each Erlang process independently, making the VM release resource sooner after finishing task and avoid stop-the-world freeze on applications as much as possible. 

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.6]{cf_min_latency_2cores}
\includegraphics[scale=0.6]{cf_min_latency_4cores}
\includegraphics[scale=0.6]{cf_min_latency_8cores}
\includegraphics[scale=0.6]{cf_min_latency_16cores}
\caption[Minimum latency of Californium on an AWS instance with increasing capability]{Minimum latency of Californium on an AWS instance with increasing capability: 2 cores, 4 cores, 8 cores and 16 cores from top to bottm}
\label{fig:cf_min_latency}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.6]{ecoap_min_latency_2cores}
\includegraphics[scale=0.6]{ecoap_min_latency_4cores}
\includegraphics[scale=0.6]{ecoap_min_latency_8cores}
\includegraphics[scale=0.6]{ecoap_min_latency_16cores}
\caption[Minimum latency of ecoap on an AWS instance with increasing capability]{Minimum latency of ecoap on an AWS instance with increasing capability: 2 cores, 4 cores, 8 cores and 16 cores from top to bottom}
\label{fig:ecoap_min_latency}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.6]{cf_max_latency_2cores}
\includegraphics[scale=0.6]{cf_max_latency_4cores}
\includegraphics[scale=0.6]{cf_max_latency_8cores}
\includegraphics[scale=0.6]{cf_max_latency_16cores}
\caption[Maximum latency of Californium on an AWS instance with increasing capability]{Maximum latency of Californium on an AWS instance with increasing capability: 2 cores, 4 cores, 8 cores and 16 cores from top to bottom}
\label{fig:cf_max_latency}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.6]{ecoap_max_latency_2cores}
\includegraphics[scale=0.6]{ecoap_max_latency_4cores}
\includegraphics[scale=0.6]{ecoap_max_latency_8cores}
\includegraphics[scale=0.6]{ecoap_max_latency_16cores}
\caption[Maximum latency of ecoap on an AWS instance with increasing capability]{Maximum latency of ecoap on an AWS instance with increasing capability: 2 cores, 4 cores, 8 cores and 16 cores from top to bottom}
\label{fig:ecoap_max_latency}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.6]{cf_95p_latency_2cores}
\includegraphics[scale=0.6]{cf_95p_latency_4cores}
\includegraphics[scale=0.6]{cf_95p_latency_8cores}
\includegraphics[scale=0.6]{cf_95p_latency_16cores}
\caption[95 percentile latency of Californium on an AWS instance with increasing capability]{95 percentile latency of Californium on an AWS instance with increasing capability: 2 cores, 4 cores, 8 cores and 16 cores from top to bottom}
\label{fig:cf_95p_latency}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.6]{ecoap_95p_latency_2cores}
\includegraphics[scale=0.6]{ecoap_95p_latency_4cores}
\includegraphics[scale=0.6]{ecoap_95p_latency_8cores}
\includegraphics[scale=0.6]{ecoap_95p_latency_16cores}
\caption[95 percentile latency of ecoap on an AWS instance with increasing capability]{95 percentile latency of ecoap on an AWS instance with increasing capability: 2 cores, 4 cores, 8 cores and 16 cores from top to bottom}
\label{fig:ecoap_95p_latency}
\end{figure}

As mentioned before, Erlang uses various strategies to achieve soft-realtime, which has the most obvious impact on the latency of a high concurrency server. \autoref{fig:cf_min_latency} and \autoref{fig:ecoap_min_latency} presents the minimum round-trip time with respect to increasing concurrency factor and increasing instance capability. Both implementations give low enough latency while Californium has slightly better results. ecoap shows a more obvious trend that the latency decreases when more CPU cores are available. However, as \autoref{fig:cf_max_latency} and \autoref{fig:ecoap_max_latency} indicate, the maximum round-trip time starts to vary. The maximum round-trip time of ecoap is more centralized and gradually grows with the number of concurrent clients, while Californium has a more scattered result that is usually an order of magnitude larger than ecoap. With more CPU cores, the maximum latency of both servers generally goes down as expected. But even with 16 cores, Californium still has maximum latency over 1 second from time to time whereas ecoap controls all latency below 300 milliseconds under the same condition. The difference of the results might also be related to the difference of garbage collection and memory management of the two implementations.

\autoref{fig:cf_95p_latency} and \autoref{fig:ecoap_95p_latency} gives the 95 percentile latency of the two servers. The 95 percentile latency gives the value that is larger than 95\% of the entire dataset and is therefore a better metrics than pure average. As seen in the figures, both datasets have a trend which increases almost linearly as the concurrency level goes up (since the x-axis is of logarithmic scale). The latency values saturate at high concurrency factors (after 2,000 clients) because network limit has been reached. This is confirmed as message timeouts appear around the same point. An interesting fact is though Californium gives better latency than ecoap at beginning, it does not improve a lot when the capability of underlying instance gets improved. On the other side, ecoap has a more predictable result where latency clearly drops given more computing power. When 16 CPU cores are provided, Californium ends up with 40 milliseconds and ecoap ends up with around 30 milliseconds. The 95 percentile latency proves both implementations can satisfy the timing requirements of the majority of tasks. Nevertheless, the Erlang runtime ensures ecoap has a more predictable behaviour, especially with worst cases.

It can be seen that the proposed server prototype has comparable performance in terms of throughput and latency to the mainstream implementation, Californium, when being deployed under cloud environment. The ecoap server scales well with increasing CPU cores and shows an advantage when general low latency is desired.


%\begin{figure}[!htbp]
%\centering
%\includegraphics[scale = 0.8]{vertical_scalability}
%\caption[Throughput for different numbers of assigned CPU cores on a 8 core m4.2xlarge AWS instance]{Throughput for different numbers of assigned CPU cores on a 8 core m4.2xlarge AWS instance. The server stay stable over an increasing concurrency factor with low latency.}
%\label{fig:scalability}
%\end{figure}

%\section{Throughput Verification}

%In this section, the ecoap server prototype is compared to Californium (release 1.0.6). Due to various limitations, the evaluation does not include other implementations. However, as a mainstream solution, Californium is proved to scale better than the majority implementations that are publicly available \autocite{lanter2013scalability}\autocite{kovatsch2014californium}\autocite{kovatsch2015scalable}. Therefore, comparing the proposed implementation with Californium should be enough to evaluate its performance. 

%\subsection{Unconstrained Environment}

%First comes the evaluation in unconstrained environment. The experiment is taken in the same environment as the the previous one, that is, a m4.4xlarge instance with 16 virtual CPUs and 64 GB RAM as client and a m4.2xlarge instance with 8 virtual CPUs and 32 GB RAM as server. Same benchmark resource is hosted on both servers and the message lifetime is also tweaked for Californium. Maximum number of open file descriptors is increased to allow enough open UDP sockets. The socket buffer is increased to 1MB for both servers to reduce unnecessary message loss. The Erlang runtime is ordered to bind all schedulers to available CPU cores and use kernel poll provided by the operating system. 

%\begin{figure}[!htbp]
%\centering
%\includegraphics[scale = 0.8]{throughput}
%\caption[Throughput comparison on a 8 core m4.2xlarge AWS instance]{Throughput comparison on a 8 core m4.2xlarge AWS instance. ecoap has same level of performance as Californium with slightly better peak throughput. Throughput of ecoap grows slower due to possible scheduling overhead introduced by the Erlang runtime. Californium suffers from a higher standard deviation, though.}
%\label{fig:throughput}
%\end{figure}

%As seen in \autoref{fig:throughput}, ecoap and Californium have close performance in terms of requests handled per second. Californium performs better at beginning than ecoap, which can be explained as the Erlang runtime brings more overhead for scheduling when there are not enough incoming requests to fully utilize all the schedulers. It increases the time processing a single request. Since the benchmark tool takes the same method as CoAPBench, the clients would wait for a request to finish before issuing the next one, making them spending more time waiting for the response. ecoap catches up at concurrency level of 40 and the throughput of both servers keep growing with the increasing number of concurrent clients. Californium stabilizes with around 150 clients while ecoap reaches its maximum capability after reaching 500 clients. ecoap has slightly better peak throughput at 95,172 requests per second versus 91,169 requests per second for Californium. Again, ecoap grows more slowly and requires more clients to saturate the server.

%On the other hand, Californium has a high standard deviation during the test, as indicated by the error bars in the figure. The standard deviation is shown for ecoap as well, but is almost negligible. The performance curve of Californium is in general not as smooth as ecoap.
%This may have many reasons. After observing the system resource consumption during the experiments, it is found that the Erlang runtime occupies more CPU and saturate faster than the Java Virtual Machine (JVM), while the JVM frees memory much slower than the Erlang one. As a result, Californium consumes much more memory than ecoap especially after long time testing. It is confirmed through both the Linux process viewer \verb|htop| and the \verb|jvisualvm| \autocite{jvisualvm} virtual machine profiling tool. There is no related memory leak being observed though, as local test proved that message exchange states are successfully removed after corresponding lifetime. It is inferred that the JVM needs more time to invoke a full garbage collection with a large RAM, meanwhile the high concurrency level leads to large amount of objects being created and deleted frequently, which eventually influenced the performance of the server. In addition, the virtual environment on AWS might have undesired impact on the experiment. In contrast, the Erlang VM uses a per process generational garbage collection mechanism which runs inside each Erlang process independently, making the VM release resource sooner after finishing task and avoid stop-the-world freeze on applications as much as possible. 

%\begin{figure}[!htbp]
%\centering
%\includegraphics[scale = 0.8]{message_loss_rate}
%\caption{Message timeouts by ecoap and Californium on a 8 core m4.2xlarge AWS instance}
%\label{fig:message_loss_rate}
%\end{figure}

%\autoref{fig:message_loss_rate} gives the message loss through timeouts waiting for responses. With 2500 concurrent clients stressing the server, timeouts start to occur. The timeout rates rises with the increase of the concurrency factor to a maximum of about 0.7\% at 10,000 concurrent clients. Compared with the total amount of requests successfully served, the message loss rate is low enough and has negligible influence on the experiment . 

%As mentioned before, Erlang uses various strategies to achieve soft-realtime, including the per process garbage collection. This has the most obvious impact on the latency of a high concurrency server. \autoref{fig:ecoap_min_round_trip_time} and \autoref{fig:californium_min_round_trip_time} present the minimum round-trip time with respect to increasing concurrency factor, respectively. Though ecoap has more scattered results, both implementation get the response time around 2 milliseconds at maximum. However, as \autoref{fig:ecoap_max_round_trip_time} and \autoref{fig:californium_max_round_trip_time} show, the maximum round-trip time starts to vary.
%The maximum round-trip time for ecoap is more concentrated and generally grows with the number of concurrent clients. The top value is around 300 milliseconds at concurrency level of 5000, which should be an acceptable delay for many time demanding applications. The maximum latency for Californium does not have an obvious rule and is usually an order of magnitude larger than ecoap. Some of them would cause a retransmission if it occurs in a real world use case. The phenomenon might be related to the high standard deviation discussed before, that is, due to garbage collection and memory management. 

%But Californium does not always gives high latency. \autoref{fig:ecoap_95p_round_trip_time} and \autoref{fig:californium_95p_round_trip_time} show the 95 percentile latency for both servers. The 95 percentile latency gives the value that is larger than 95\% of the entire dataset and is therefore a better metrics than pure average. As seen in the figures, both dataset increases almost linearly as the concurrency level goes up (since the x-axis is of logarithmic scale). Californium has a smaller maximum value of around 30 milliseconds after 2500 concurrent clients versus 35 milliseconds for ecoap also after 2500 clients. The latency values saturate at the same place because the network limit has been reached, since the timeouts also start to appear after this point. It seems that Californium could respond more requests only with a short delay than ecoap if one only considers the 95 percentile latency. However, the fair scheduling of Erlang ensures ecoap performs better in general especially with worst cases.

%\begin{figure}[!htbp]
%\centering
%\includegraphics[scale = 0.8]{ecoap_min_round_trip_time}
%\caption{Minimum round trip time of ecoap running on a 8 core m4.2xlarge AWS instance}
%\label{fig:ecoap_min_round_trip_time}
%\includegraphics[scale = 0.8]{californium_min_round_trip_time}
%\caption{Minimum round trip time of Californium running on a 8 core m4.2xlarge AWS instance}
%\label{fig:californium_min_round_trip_time}
%\end{figure}

%\begin{figure}[!htbp]
%\centering
%\includegraphics[scale = 0.8]{ecoap_max_round_trip_time}
%\caption{Maximum round trip time of ecoap running on a 8 core m4.2xlarge AWS instance}
%\label{fig:ecoap_max_round_trip_time}
%\includegraphics[scale = 0.8]{californium_max_round_trip_time}
%\caption{Maximum round trip time of Californium running on a 8 core m4.2xlarge AWS instance}
%\label{fig:californium_max_round_trip_time}
%\end{figure}

%\begin{figure}[!htbp]
%\centering
%\includegraphics[scale = 0.8]{ecoap_95p_round_trip_time}
%\caption{95 percentile round trip time of ecoap running on a 8 core m4.2xlarge AWS instance}
%\label{fig:ecoap_95p_round_trip_time}
%\includegraphics[scale = 0.8]{californium_95p_round_trip_time}
%\caption{95 percentile round trip time of Californium running on a 8 core m4.2xlarge AWS instance}
%\label{fig:californium_95p_round_trip_time}
%\end{figure}

\subsection{Constrained Environment}

It is interesting to see to what extend a solution could scale up and scale down when being targeted to platforms with great capability difference. Hence, a similar experiment is conducted under a more constrained environment, the Raspberry Pi 3 \autocite{raspberry_pi}. The Raspberry Pi is essentially an embedded computer which can run Linux compatible applications. It is never as constrained as sensors and low-power devices, however, such type of platforms are still widely used as gateways or local processing unit in many IoT applications. For instance, it is ideal for running a Fog node that encapsulates more dummy sensors and devices.

\begin{figure}[!htbp]
\centering
\includegraphics[scale = 0.6]{experiment_setting_rasp}
\caption{Experiment setup in constrained environment}
\label{fig:experiment_setting_rasp}
\end{figure}

The Raspberry Pi 3 is equipped with a quad-core 1.2GHz Broadcom BCM2837 64bit CPU, with 1GB RAM and BCM43438 wireless LAN and Bluetooth Low Energy (BLE) on board. The experiment environment consists of one Pi running the CoAP server and a MacBook Pro running the benchmark tool, connected via a Gigabyte network switch, as shown in \autoref{fig:experiment_setting_rasp}. It is considered the MacBook Pro is powerful enough to generate loads that can saturate the Pi. 

The test measures the average throughput as well as latency of each server under stress, starting with 1 client and stepwise increasing the concurrency level to 10,000. It is of little interest to explore the vertical scalability on such a constrained platform. Therefore the evaluation is done with a fixed number of CPU cores. All other settings including the server tweaking and socket buffer tuning are the same as the AWS experiment.

\begin{figure}[!htbp]
\centering
\includegraphics[scale = 0.7]{throughput_rasp}
\caption{Throughput on a quad-core Raspberry Pi 3 at 1.2 GHz and 1GB RAM}
\label{fig:throughput_rasp}
\end{figure}

\autoref{fig:throughput_rasp} shows the throughput of the two servers on the Raspberry Pi. The performance curves are not as stable as the ones under cloud environment. This might be due to the limitation of the processing power of the Pi. ecoap achieves highest throughput with about 9,000 requests per second. Interestingly, throughput of Californium increases rapidly at low concurrency level but soon decreases afterwards. The standard deviation is much higher than before, which implies it does not fully stabilize during the test. Californium eventually exits with an out of memory exception at the concurrency level of 4,500. Without further investigation, it is not obvious why Californium crashes during the test. It is clearly not designed for constrained environment. But as stated in its introduction, Californium is also capable of running on embedded platforms such as an Android smartphone, which provides similar processing ability as the Raspberry Pi. Again, it is most likely that without further tuning and optimization, the underlying JVM can not manage memory efficiently within such a limited RAM under high concurrency load. Several proofs can be found here. It is observed during the test that the JVM on Raspberry Pi starts in client mode by default, which renders the server crash at even lower concurrency level. Setting the VM to server mode and increase the heap size helps the server to run longer, but 4,500 clients is the final limitation here. 

\begin{figure}[!htbp]
\centering
\includegraphics[scale = 0.7]{ecoap_min_latency_rasp}
\caption{Minimum latency of ecoap on Raspberry Pi 3}
\label{fig:ecoap_min_latency_rasp}
\includegraphics[scale = 0.7]{californium_min_latency_rasp}
\caption{Minimum round trip time of Californium on Raspberry Pi 3}
\label{fig:californium_min_latency_rasp}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[scale = 0.7]{ecoap_max_latency_rasp}
\caption{Maximum latency of ecoap on Raspberry Pi 3}
\label{fig:ecoap_max_latency_rasp}
\includegraphics[scale = 0.7]{californium_max_latency_rasp}
\caption{Maximum latency of Californium on Raspberry Pi 3}
\label{fig:californium_max_latency_rasp}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[scale = 0.7]{ecoap_95p_latency_rasp}
\caption{95 percentile latency of ecoap on Raspberry Pi 3}
\label{fig:ecoap_95p_latency_rasp}
\includegraphics[scale = 0.7]{californium_95p_latency_rasp}
\caption{95 percentile latency of Californium on Raspberry Pi 3}
\label{fig:californium_95p_latency_rasp}
\end{figure}

The general result resembles the trend of the previous experiment. \autoref{fig:ecoap_min_latency_rasp} and \autoref{fig:californium_min_latency_rasp} show the minimum latency clients can achieve in the experiment. When concurrency level is high, both servers need around 20-30 milliseconds at least to process a request, which is reasonably much slower than the cloud.
On the other hand, \autoref{fig:ecoap_max_latency_rasp} indicates similar trend as the maximum latency of ecoap derived form the cloud experiment, though the largest value already exceeds 600 milliseconds. However, it is still more responsive compared to Californium, whose maximum latency is commonly over 2 seconds, as seen in \autoref{fig:californium_max_latency_rasp}. When it comes to 95 percentile latency in \autoref{fig:ecoap_95p_latency_rasp} and \autoref{fig:californium_95p_latency_rasp}, performance of Californium is still acceptable, sometimes better than ecoap, until it hits a high concurrency factor. The ecoap server keeps stable during the intense stressing test with on average 3,000 more requests handled per second than Californium. Most of the requests are responded within 1 second even handling 10,000 concurrent clients.

To better illustrate the soft real-time characteristics of Erlang, a separate resource \verb|\fibonacci| is hosted on the Raspberry Pi, which computes a fibonacci number in a highly inefficient recursive way. The input parameter of the computation is carried in the request query. Then 100 virtual clients would send requests to the \verb|\fibonacci| resource in order to quickly saturate the server. Meanwhile another 100 clients would send requests to the \verb|\benchmark| resource expecting responses as usual. Statistics of the non CPU intensive requests are given in \autoref{tab:rasp_fib}. For Californium, all requests end up with timeout since the underlying threads that executes the CPU-intensive task block the whole server. However, under same load ecoap is still accessible for the non CPU-intensive requests, at the cost of increasing response time and possible more timeout messages (though not shown in this test). It is the preemptive scheduling of Erlang that ensures no process should occupy too much CPU time and improves overall responsiveness. It is of no doubt that when eventually all CPU resource has been consumed, no server could process requests normally any more. That being said, the Erlang runtime still improves the robustness of an application under heavy load.

\begin{table}[!htbp]
\centering
\begin{tabular}{llllll}
%
 & \bfseries throughput &  \bfseries timeout & \bfseries min latency & \bfseries max latency & \bfseries 95p latency \\\hline
\bfseries Californium & 0.00 & 100\% & 0.00 & 0.00 & 0.00 \\\hline
\bfseries ecoap & 404.93 & 0\% & 13.98 & 7405.57 & 546.00\\
\end{tabular}
\caption[Result of fibonacci test on Raspberry Pi 3]{Result of fibonacci test on Raspberry Pi 3 which shows statistics of the requests hitting the benchmark resource. The throughput is measured as processed requests per second. Timeout shows the percentage of lost messages in all sent messages. All latency is measured in milliseconds. Californium is not responsive so no meaningful latency is recorded while ecoap is still available at the cost of degraded performance.}
\label{tab:rasp_fib}
\end{table}

The concurrency model behind ecoap allows it to scale down to embedded platforms such as the Raspberry Pi without a problem. Since all concurrent activities are modelled as lightweight isolated processes, unnecessary synchronizations are avoided and the underlying runtime could schedule and manage the processes in a consistent manner. As a result, the benefits of fair scheduling and independent garbage collection that are discussed in unconstrained experiment also apply here and have more explicit effects. Though other solutions can be highly customizable as well, for example, Californium supports setting different number of threads in each processing stage. But as what the experiment results show, the limitations of the underlying concurrency model still reduces the overall flexibility. 

\section{Fault-Tolerance Test}

Fault-tolerance is another design goal of ecoap. In order to verify the behaviour under various faults and failures, the faults need to be injected to a running server in a random manner. Chaos Monkey is a resiliency tool that helps applications tolerate random failures \autocite{chaos_monkey_netflix}. It originates from Netflix and aims at testing the fault-tolerance of a a production environment by randomly terminating virtual machine instances and containers. That being said, this tool works at a coarse-grained level and does not provide finer control within one application instance. However, one Erlang application inspired by Chaos Monkey can do the trick. The application which is also called chaos monkey \autocite{chaos_monkey_erl} can randomly terminate an Erlang process within another application at pre-defined rate. It therefore effectively tests the stability of the target application with any combination of failures. 

The experiment is conducted on an AWS m4.2xlarge instance with 8 vCPUs and 32 GB of RAM with other settings the same as the previous AWS tests. To evaluate how fault-tolerant ecoap is, the experiment first starts the server and the benchmark tool as normal, and then randomly kills a process of the server every 2 seconds during an ongoing stress test. The interval of 2 seconds is the minimum value that can be used respecting the restart limit of supervisors inside ecoap. The chaos monkey is smart enough to only kill worker processes because it is where the majority of errors happen, while the supervisors are too strong to kill. Usually the crash of a supervisor implies an unrecoverable error just occurred and the entire supervision tree is abandoned. 

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.7]{ecoap_faults_throughput}
\caption[Throughput of ecoap with faults injected on an 8-core AWS instance]{Throughput of ecoap with faults injected on an 8-core AWS instance: The chaos monkey application randomly kill a worker process of ecoap every 2 seconds. The performance is more disturbed at low concurrency factor.}
\label{fig:ecoap_faults_throughput}
\end{figure}

In such a way, the throughput of the server under ``attack" is shown in \autoref{fig:ecoap_faults_throughput}. It can be seen clearly that when the chaos monkey application is active, the standard deviation gets quite large at low concurrency level. This is because with low concurrency factor, fewer processes run in the server and killing one of them every 2 seconds is considered a non-negligible obstacle. The server survives the attack but can not run as stable as usual. Nevertheless, the situation gets better when more concurrent clients are involved, since a system with larger amount of processes is naturally resilient to fixed rate failure. Therefore, the throughput gets closer to the one without any fault as the number of concurrent clients grows. 

\begin{figure}[!htbp]
\centering
\includegraphics[scale=0.7]{ecoap_faults_min_latency}
\caption{Minimum latency of ecoap with faults injected on an 8-core AWS instance}
\label{fig:ecoap_faults_min_latency}
\includegraphics[scale=0.7]{ecoap_faults_max_latency}
\caption{Maximum latency of ecoap with faults injected on an 8-core AWS instance}
\label{fig:ecoap_faults_max_latency}
\includegraphics[scale=0.7]{ecoap_faults_95p_latency}
\caption{95 percentile latency of ecoap with faults injected on an 8-core AWS instance}
\label{fig:ecoap_faults_95p_latency}
\end{figure}

\autoref{fig:ecoap_faults_min_latency}, \autoref{fig:ecoap_faults_max_latency} and \autoref{fig:ecoap_faults_95p_latency} show the minimum, maximum and 95 percentile latency during the test respectively. It should be noticed that compared with the data with no fault, only slight difference exists. The server is overall not greatly influenced by the injected faults. On the other hand, message timeout rate does not provide much information here. Though with injected faults there are message timeouts from the beginning of the test, the timeout percentage is in general similar to normal one or even lower. This is reasonable because the virtual clients work in a closed-loop fashion. The clients send fewer requests in total as a result of the faults, so as the number of timeout messages. Thus the percentage of lost messages will not change dramatically.

It is interesting enough to see that ecoap can keep providing service with such a high failure rate. Moreover, since the killed process is randomly selected, the experiment should prove that the system is recoverable and resilient no matter which stage went wrong. Without Erlang's share-nothing isolation and supervision tree mechanism, this fine-grained fault-tolerance is hard to achieve. It can be perceived that the high reliability of single node would make it easier to build a high availability multi-node system.

Similar tests can not be conducted with Californium, as it makes no sense to randomly terminate a thread within a threading concurrency model. It also means that in order to improve reliability, application with traditional model must use defensive programming, carefully managed locks and asynchronous calls. While this can fit specific circumstances very well and outperform the proposed concurrency model, however, it is essentially not as intuitive as the latter and comes with much more maintainability overhead. 

%\begin{figure}[!htbp]
%\centering
%\includegraphics[scale = 0.8]{throughput_with_faults}
%\caption[Throughout of ecoap with randomly injected faults]{Throughout of ecoap with randomly injected faults. The chaos monkey application randomly kill a worker process of ecoap every 2 seconds. The performance is more disturbed at low concurrency factor.}
%\label{fig:throughput_with_faults}
%\end{figure}


%\begin{figure}[!htbp]
%\centering
%\includegraphics[scale = 0.8]{ecoap_min_round_trip_time_faults}
%\caption{Minimum round trip time of ecoap with randomly injected faults}
%\label{fig:ecoap_min_round_trip_time_faults}
%\end{figure}

%\begin{figure}[!htbp]
%\centering
%\includegraphics[scale = 0.8]{ecoap_max_round_trip_time_faults}
%\caption{Maximum round trip time of ecoap with randomly injected faults}
%\label{fig:ecoap_max_round_trip_time_faults}
%\end{figure}

%\begin{figure}[!htbp]
%\centering
%\includegraphics[scale = 0.8]{ecoap_95p_round_trip_time_faults}
%\caption{95 percentile round trip time of ecoap with randomly injected faults}
%\label{fig:ecoap_95p_round_trip_time_faults}
%\end{figure}

%Add “A note of horizontal scalability”
%1. Commonly used pattern - load balancer + multiple servers
%2. One may eager to/aggressive to use erlang distribution to distribute work load among multiple nodes, however this does apply to our situation well
%3. Illustrate one possibility - one server routing requests to others and forward them back, and its limitations
%4. Give data of peak performance of the two approaches (load balancer & ad-hoc) on Google Compute Could 
%5. Reference to Leshan clustering wiki page (what people currently do for horizontally scale a CoAP based solution and support our point of view)
%6. (Auto scale out and scale in challenge?) WebRTC shares some similarities with CoAP: real time, stateful services, UDP based, (streaming?) - the techniques of scaling WebRTC might be inspiring to scaling CoAP - add reference to https://webrtchacks.com/webrtc-media-servers-in-the-cloud/

\section{Discussion on Horizontal Scalability} \label{horizontal_scale_note}

Though one of the primary goals of this work is to verify vertical scalability of the proposed architecture, it is of interest how a language with transparent distribution built-in like Erlang deals with horizontal scalability. However, what Erlang provides are general tools for building distributed applications while the usage and range of application vary a lot. 

When it comes to a CoAP server, firstly common patterns for scaling web servers should apply, such as adding a load balancer in front a farm of servers. Since this approach does not require synchronization among different server instances, all servers can simply run concurrently and therefore maximizing the performance benefit. It should be noted that one difference between CoAP servers and HTTP servers is some extensions of CoAP are essentially stateful. For example, observe and block-wise transfer both require maintaining states on server side. If the implementation does not employ certain type of distributed storage then these states have be local on each server. Therefore a load balancer that supports ``sticky'' mode is preferred, since it can forward subsequent requests from one client always to the same target server. 

For ecoap, block-wise GET requests can still work without the support of ``sticky'' mode because it is a safe operation \autocite{coap_protocol}. The downside is reduced efficiency as multiple copies of the resource might be read by a number of servers. Block-wise PUT/POST can not work without this support due to the fact that they are implemented following the atomic fashion \autocite{blockwise}. Similarly, an extra deduplication mechanism has to be provided for observe to avoid duplicate observe relations being established over multiple servers. 

During the experiments of this work, a preliminary attempt was made to utilize the transparent distribution of Erlang to dispatch requests instead of using a load balancer. The attempt involves a small modification to the server so that it can act as a dispatcher while still being a functional server. It is simply done by exposing the API for starting the sub supervision tree that represents a remote endpoint. After starting multiple Erlang VMs each on a different machine and connecting them into a mesh network using the tool provided by the runtime, one can have the ecoap application running on each node. Then one of the server instance can act as the entry point and receive incoming requests. For any new client, the socket manager on that node call the exposed API to start the endpoint supervision tree randomly on one of the connected nodes and forward the request to it, just as it does with a local endpoint supervision tree. After requests have been processed, all responses have to be send back the entry socket manager, which then send them over network. This is because socket can not be shared among nodes. Despite this, all other functionalities that use message passing underneath work in the same way as before. Because of the fact that Erlang uses same model for concurrency and distribution,
the above modification does not include any large refactoring but manages to dispatch requests evenly among many servers.

However some simple tests immediately reveal limitation of this approach. In order to have a comparison with the load balancer approach, all tests are conducted on Google Compute Engine \autocite{google_compute_engine} since it supports built-in UDP load balancing while AWS does not. The virtual machine instance for test consists of 2 CPU cores and 8 GB RAM. Initial tests show that with load balancing, ecoap server gives throughput of around 2,3000 requests/second with one instance, around 4,5000 requests/second with two instances and around 7,2000 requests/second with three instances, all stressed with 1,000 concurrent clients using the Erlang CoAP benchmark tool running on another powerful instance. As expected throughput increases almost linearly with this approach. While with the server-dispatching approach, the throughput goes to 2,2000 requests/second with one instance, 3,1000 requests/second with two instances and 3,5000 requests/second with three instances. The second approach is clearly less performant and the bottleneck is obvious: the single socket manager still has to handle all network lifting (even worse than usual since it also handles outgoing traffic). Furthermore, Erlang messages that are sent to remote process involves network latency, which has to be counted for every request whose processing logic is non-local.

It is considered that without more complex business logic, it is of little benefit to scale the CoAP server out utilizing distributed Erlang. In such a case, using load balancer directly is more efficient. Nonetheless, distributed Erlang can largely simplify application logic when the system does involve many different components. For instance, a real world case might use the server in pair with mnesia \autocite{mnesia}, the Erlang's built-in database that can be configured to replicate its state over multiple nodes for scalability and reliability using nothing else but pure distributed Erlang. 

Since CoAP is a relatively young protocol, clustering of CoAP services are still under research. Leshan \autocite{leshan} is an OMA Lightweight M2M (LWM2M) implementation in Java. It is built on top of the Californium framework. There is a demand for using Leshan servers in a cluster and it faces similar challenges as it uses CoAP underneath. The wiki page of the project \autocite{leshan_cluster} states information and concerns for CoAP clustering, which suggests that there is still much room for improvement in this area.

\section{Summary}

This chapter answers the question how CoAP can be efficiently implemented using a concurrency-oriented language like Erlang, and how the system can automatically scale up to powerful cloud environment as well as scale down to constrained platform. The ecoap prototype gives an architecture for scalable and reliable IoT services. Furthermore, a detailed evaluation of the proposed prototype in both constrained and unconstrained environment is presented. The ecoap prototype consists of reusable components for both server and client, which then forms a flexible concurrency model where protocol processing is clearly divided and parallelized. Details of different components and how they work together are also explained in this chapter, with an emphasis on modelling and managing various states efficiently within CoAP constraints. The evaluation shows that the concurrency model fits modern multi-core systems well. The implementation has similar or better performance compared with mainstream CoAP solution. 

In unconstrained environment, ecoap has comparable level of scalability as Californium with respect to a growing number of concurrent clients. Both implementations maintain a high throughout even with high concurrency factor and only latency increases. However, ecoap delivers in general more stable and responsive performance as a result of the underlying concurrency model. The latency results indicate that ecoap is more suitable for applications with real-time requirements, since it ensures the worst cases are still within an acceptable range. Furthermore, ecoap shows overall better scalability with respect to increasing computing power, which is a more valuable feature in multi-core and many-core era.

In constrained environment, ecoap outperforms Californium in both throughput and latency. It automatically scales down without extensive optimization while keeping consistent behaviour as in unconstrained environment. Other solutions may have a concurrency model that is more advanced in certain scenarios, however, they might not benefit from it when the environment changes. ecoap and its concurrency model gain more flexibility in general.

Moreover, ecoap is more resilient in terms of unexpected faults and failures. The fine-grained control over errors and faults and recover process can not be easily achieved in other solutions. 
As the number of participants of the IoT increases rapidly, such feature becomes more important because it gives more alternatives to achieve reliability. 



%what can the readers learn from your work -> the real conclusion!
